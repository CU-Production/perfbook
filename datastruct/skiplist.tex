% datastruct/skiplist.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{chp:Data Structures: Skip lists}{Data Structures: Skip Lists}{qqzdatastructskiplist}
%
\Epigraph{I think that I shall never see \\
	  A poem lovely as a tree.}
	 {Joyce Kilmer}

The preceding chapter focused on data structures that enhance
concurrency due to partitionability
(\cref{sec:datastruct:Partitionable Data Structures}
on
\cpageref{sec:datastruct:Partitionable Data Structures}),
efficient handling of read-mostly access patterns
(\cref{sec:datastruct:Read-Mostly Data Structures}
on
\cpageref{sec:datastruct:Read-Mostly Data Structures}),
or application of read-mostly techniques to avoid
non-partitionability
(\cref{sec:datastruct:Non-Partitionable Data Structures}
on
\cpageref{sec:datastruct:Non-Partitionable Data Structures}).
One of the hash table's greatest advantages for parallel use is that it
is fully partitionable, at least while not being resized.
However, it does not preserve locality of reference (unless said
locality is hash-function-aware) and it does not support key-order
traversals.

One way of gaining locality of reference and key-order traversal
to use a tree.
But use of trees in concurrent software raises some questions:

\begin{enumerate}
\item	Can trees support concurrent updates, which is trivially
	provided by hash tables?
\item	Trees avoid hash-table resizes, but often require
	rebalancing.
	Can this rebalancing be avoided?
\item	If rebalancing is necessary, can it be carried out while
	preserving performance and scalability for both readers
	and updaters?
\end{enumerate}

One such tree is the radix tree, which is also called a trie.
Tries partition the search key, using each successive key partition
to traverse the next level of the trie.
As such, a trie can be thought of as a set of nested hash tables,
thus providing the required concurrent updates.
Although tries do not require rebalancing, a sparse key space can result
in inefficient use of memory.
There are a number of compression techniques that may be used to
work around this disadvantage, including hashing the key value to
a smaller keyspace before the
traversal~\cite{RobertOlsson2007Trash}.
Radix trees are heavily used in practice, including in the Linux
kernel~\cite{NickPiggin2006radixtree}.
However, because of their similarity to hash tables, this chapter
will not say more about them.

One important special case of both a hash table and a trie is what is
perhaps the oldest of data structures, the array and its multi-dimensional
counterpart, the matrix.
The fully partitionable nature of matrices is exploited heavily in
concurrent numerical algorithms, as we saw in
\cref{sec:SMPdesign:Locking Granularity and Performance}
on
\cpageref{sec:SMPdesign:Locking Granularity and Performance}.
This chapter will nevertheless focus on linked data structures.

Self-balancing trees are heavily used in sequential code, with
AVL trees and red-black trees being perhaps the most well-known
examples~\cite{ThomasHCorman2001Algorithms}.
Early attempts to parallelize AVL trees were complex and not necessarily
all that efficient~\cite{Ellis80},
however, more recent work on red-black trees provides better
performance and scalability by using RCU for readers and hashed arrays
of locks\footnote{
	In the guise of swissTM~\cite{AleksandarDragovejic2011STMnotToy},
	which is a variant of software transactional memory in which
	the developer flags non-shared accesses.}
to protect reads and updates,
respectively~\cite{PhilHoward2011RCUTMRBTree,PhilipWHoward2013RCUrbtree}.
It turns out that red-black trees rebalance aggressively, which works
well for sequential programs, but not necessarily so well for parallel
use involving frequent updates.
Recent work has therefore made use of RCU-protected ``bonsai trees''
that rebalance less aggressively~\cite{AustinClements2012RCULinux:mmapsem},
trading off optimal tree depth to gain more efficient concurrent updates.
However, efficiently and scalably handling rebalancing is still somewhat
of a research topic, so this chapter will look elsewhere.

An RCU-protected B-tree variant, called
``Maple Tree''~\cite{LiamHowlett2022MapleTree,MatthewWilcox2022MapleTree},
has been implemented within the Linux kernel.
It is perhaps most notably used for tracking per-process address spaces.
But useful though Maple Tree is, it is not exactly a textbook algorithm.

In contrast, concurrent skip lists are quite simple and also (like Maple
Trees) lend themselves well to concurrent rebalancing, to RCU readers,
and to concurrent updates.
In fact skiplists were an early academic use of a technique resembling
RCU~\cite{Pugh90}.
This chapter will therefore focus on concurrent RCU-protected skip lists.

@@@
Because this chapter cannot delve into the details of every concurrent
data structure,
\cref{sec:skiplist:Other Data Structures}
surveys a few of the important ones.
@@@
\section{Skip List Design}
\label{sec:skiplist:Skip List Design}
%
\epigraph{Skip, skip, skip to my Lou}
	 {Unknown}

A skip list uses multiple links in each element in order to
provide shortcuts through a linked list, as depicted in
\cref{fig:datastruct:Skip List Diagram}.
These shortcuts allow a lookup to reach the desired element with
at most two pointer traversals.

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{datastruct/skiplistlayout}}
\caption{Skip List Diagram}
\label{fig:datastruct:Skip List Diagram}
\end{figure}

\QuickQuiz{
	But skip lists are not trees, but instead directed acyclic
	graphs, aren't they now???
}\QuickQuizAnswer{
	Well spotted!

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{datastruct/skiplisttree}}
\caption{Tree Within Skip List}
\label{fig:datastruct:Tree Within Skip List}
\end{figure}

	Except that the union of the paths traversed by readers really
	does form a tree, as you can see from the green arrows in
	\cref{fig:datastruct:Skip List Diagram}.
	Besides, a tree is a special case of a directed acyclic
	graph!
}\QuickQuizEnd

\begin{listing}
\input{CodeSamples/datastruct/skiplist/skiplist@struct.fcv}
\caption{Skip List Data Structures}
\label{lst:datastruct:Skip List Data Structures}
\end{listing}

A data structure implementing both the skip-list header and data
elements is shown in
\cref{lst:datastruct:Skip List Data Structures}.
The \co{->sl_toplevel} field indicates the array index of the uppermost
active skiplist pointer in a given element.
In the diagram, the header (leftmost) element and elements~D and~E
would have \co{->sl_toplevel} values of~2, element~B a value of~1,
and elements~A and~ a value of~0.
The array itself is in the last field, \co{->sl_next[]}.
The \co{->sl_lock} field guards all fields in the element for update,
the \co{->sl_deleted} field is used to resolve lookup/deletion races,
the \co{->sl_head} field references the header,
the \co{->sl_seq} field is used to inform readers that an update is in
progress,
and the \co{->sl_cmp()} field points to a function that does key
comparison, with the key normally being in a data structure that contains
the \co{struct skiplist}.

\QuickQuiz{
	If the \co{skiplist} structure is to be used in an
	RCU-protected linked structure, shouldn't there be an
	\co{rcu_head} structure in there somewhere?
}\QuickQuizAnswer{
	If the skip list in question is to be protected by
	RCU, then the structure in which the \co{skiplist}
	structure is enclosed must contain an \co{rcu_head}
	structure.
	And while we are at it, please note that this enclosing
	structure also contains the search key.

	This situation is similar to that of the hash-table
	elements in \cref{chp:Data Structures: Hash Tables} for
	the search key, but differs for the \co{rcu_head} structure.
}\QuickQuizEnd

\section{Other Data Structures}
\label{sec:skiplist:Other Data Structures}
%
\epigraph{All life is an experiment.
	  The more experiments you make the better.}
	 {Ralph Waldo Emerson}

Concurrent double-ended queues were discussed in
\cref{sec:SMPdesign:Double-Ended Queue},
and concurrent stacks and queues have a long history~\cite{Treiber86},
though not normally the most impressive performance or scalability.
They are nevertheless a common feature of concurrent
libraries~\cite{PaulMcKenney2013LWNURCUqueuestack}.
Researchers have recently proposed relaxing the ordering constraints
of stacks and queues~\cite{Shavit:2011:DSM:1897852.1897873},
with some work indicating that relaxed-ordered queues actually have
better ordering properties than do strict FIFO
queues~\cite{AndreasHaas2012FIFOisnt,ChristophMKirsch2012FIFOisntTR,AndreasHaas2013CFRelaxedQueues}.

It seems likely that continued work with concurrent data structures will
produce novel algorithms with surprising properties.

That said, performance and scalability are of little use without reliability,
so the next chapter covers validation.

\QuickQuizAnswersChp{qqzdatastructskiplist}
