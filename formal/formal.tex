% formal/formal.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{chp:Formal Verification}{Formal Verification}
%
\Epigraph{Beware of bugs in the above code; I have only proved it correct,
	  not tried it.}{\emph{Donald Knuth}}

\OriginallyPublished{Chapter}{chp:Formal Verification}{Formal Verification}{Linux Weekly News}{PaulEMcKenney2007QRCUspin,PaulEMcKenney2008dynticksRCU,PaulEMcKenney2011ppcmem}

Parallel algorithms can be hard to write, and even harder to debug.
Testing, though essential, is insufficient, as fatal race conditions
can have extremely low probabilities of occurrence.
Proofs of correctness can be valuable, but in the end are just as
prone to human error as is the original algorithm.
In addition, a proof of correctness cannot be expected to find errors
in your assumptions, shortcomings in the requirements,
misunderstandings of the underlying software or hardware primitives,
or errors that you did not think to construct a proof for.
This means that formal methods can never replace testing, however,
formal methods are nevertheless a valuable addition to your validation toolbox.

It would be very helpful to have a tool that could somehow locate
all race conditions.
A number of such tools exist, for example,
\cref{sec:formal:State-Space Search} provides an
introduction to the general-purpose state-space search tools Promela and Spin,
\cref{sec:formal:Special-Purpose State-Space Search}
similarly introduces the special-purpose ppcmem and cppmem tools,
\cref{sec:formal:Axiomatic Approaches}
looks at an example axiomatic approach,
\cref{sec:formal:SAT Solvers}
briefly overviews SAT solvers,
\cref{sec:formal:Stateless Model Checkers}
briefly overviews stateless model checkers,
\cref{sec:formal:Summary}
sums up use of formal-verification tools for verifying parallel algorithms,
and finally
\cref{sec:formal:Choosing a Validation Plan}
discusses how to decide how much and what type of validation to apply
to a given software project.

\input{formal/spinhint}
\input{formal/dyntickrcu}
\input{formal/ppcmem}
\input{formal/axiomatic}
\input{formal/sat}
\input{formal/stateless}

\section{Summary}
\label{sec:formal:Summary}
%
\epigraph{Western thought has focused on True-False;
	  it is high time to shift to Robust-Fragile.}
	 {\emph{summarized from Nassim Nicholas Taleb}}
% Full quote:
% Since Plato, Western thought and the theory of knowledge has focused on
% the notions of True-False; as commendable as that was, it is high time
% to shift the concern to Robust-Fragile, and social epistemology to the
% more serious problem of Sucker-Nonsucker.

The formal-verification techniques described in this chapter
are very powerful tools for validating small
parallel algorithms, but they should not be the only tools in your toolbox.
Despite decades of focus on formal verification, testing remains the
validation workhorse for large parallel software
systems~\cite{JonathanCorbet2006lockdep,DaveJones2011Trinity,PaulEMcKenney2016Formal}.

It is nevertheless quite possible that this will not always be the case.
To see this, consider that there is estimated to be more than twenty
billion instances of the Linux kernel as of 2017.
Suppose that the Linux kernel has a bug that manifests on average every million
years of runtime.
As noted at the end of the preceding chapter, this bug will be appearing
more than 50 times \emph{per day} across the installed base.
But the fact remains that most formal validation techniques can be used
only on very small codebases.
So what is a concurrency coder to do?

One approach is to think in terms of finding the first bug, the first
relevant bug, the last relevant bug, and the last bug.

The first bug is normally found via inspection or compiler diagnostics.
Although the increasingly sophisticated diagnostics provided by modern
compilers might be considered to be a lightweight sort of formal
verification, it is not common to think of them in those terms.
This is in part due to an odd practitioner prejudice which says
``If I am using it, it cannot be formal verification'' on the one
hand, and the large difference in sophistication between compiler
diagnostics and verification research on the other.

Although the first relevant bug might be located via inspection or
compiler diagnostics, it is not unusual for these two steps to find
only typos and false positives.
Either way, the bulk of the relevant bugs, that is, those bugs that
might actually be encountered in production, will often be found via testing.

When testing is driven by anticipated or real use cases, it is not
uncommon for the last relevant bug to be located by testing.
This situation might motivate a complete rejection of formal verification,
however, irrelevant bugs have an annoying habit of suddenly becoming relevant
at the least convenient moment possible, courtesy of black-hat attacks.
For security-critical software, which appears to be a continually
increasing fraction of the total, there can thus be strong motivation
to find and fix the last bug.
Testing is demonstrably unable to find the last bug, so there is a
possible role for formal verification.
That is, there is such a role if and only if formal verification
proves capable of growing into it.
As this chapter has shown, current formal verification systems are
extremely limited.

\QuickQuiz{
	But shouldn't sufficiently low-level software be for all intents
	and purposes immune to being exploited by black hats?
}\QuickQuizAnswer{
	Unfortunately, no.

	At one time, Paul E. McKenny felt that Linux-kernel RCU
	was immune to such exploits, but the advent of Row Hammer
	showed him otherwise.
	After all, if the black hats can hit the system's DRAM,
	they can hit any and all low-level software, even including RCU.

	And in 2018, this possibility passed from the realm of
	theoretical speculation into the hard and fast realm of
	objective reality~\cite{McKenney:2019:CRS:3319647.3325836}.
}\QuickQuizEnd

Another approach is to consider that
formal verification is often much harder to use than is testing.
This is of course in part a cultural statement, and there is every reason
to hope that formal verification will be perceived to be easier as more
people become familiar with it.
That said, very simple test harnesses can find significant bugs in arbitrarily
large software systems.
In contrast, the effort required to apply formal verification seems to
increase dramatically as the system size increases.

I have nevertheless made occasional use of formal verification
for almost 30 years by playing to formal verification's strengths,
namely design-time verification of small complex portions of the overarching
software construct.
The larger overarching software construct is of course validated by testing.

\QuickQuiz{
	In light of the full verification of the L4 microkernel,
	isn't this limited view of formal verification just a little
	bit obsolete?
}\QuickQuizAnswer{
	Unfortunately, no.

	The first full verification of the L4 microkernel was a tour de force,
	with a large number of Ph.D.~students hand-verifying code at a
	very slow per-student rate.
	This level of effort could not be applied to most software projects
	because the rate of change is just too great.
	Furthermore, although the L4 microkernel is a large software
	artifact from the viewpoint of formal verification, it is tiny
	compared to a great number of projects, including LLVM,
	\GCC, the Linux kernel, Hadoop, MongoDB, and a great many others.
	In addition, this verification did have limits, as the researchers
	freely admit, to their credit:
	\url{https://wiki.sel4.systems/FrequentlyAskedQuestions#Does_seL4_have_zero_bugs.3F}.

	Although formal verification is finally starting to show some
	promise, including more-recent L4 verifications involving greater
	levels of automation, it currently has no chance of completely
	displacing testing in the foreseeable future.
	And although I would dearly love to be proven wrong on this point,
	please note that such proof will be in the form of a real tool
	that verifies real software, not in the form of a large body of
	rousing rhetoric.

	Perhaps someday formal verification will be used heavily for
	validation, including for what is now known as regression testing.
	Section~\ref{sec:future:Formal Regression Testing?} looks at
	what would be required to make this possibility a reality.
}\QuickQuizEnd

One final approach is to consider the following two definitions and the
consequence that they imply:

\begin{description}[itemsep=0pt,labelindent=1em]
\item[Definition:]	Bug-free programs are trivial programs.
\item[Definition:]	Reliable programs have no known bugs.
\item[Consequence:]	Any non-trivial reliable program contains at least
			one as-yet-unknown bug.
\end{description}

From this viewpoint, any advances in validation and verification can
have but two effects: (1)~An increase in the number of trivial programs or
(2)~A decrease in the number of reliable programs.
Of course, the human race's increasing reliance on multicore systems and
software provides extreme motivation for a very sharp increase in the
number of trivial programs!

However, if your code is so complex that you find yourself
relying too heavily on formal-verification
tools, you should carefully rethink your design, especially if your
formal-verification tools require your code to be hand-translated
to a special-purpose language.
For example, a complex implementation of the dynticks interface for
preemptible RCU that was presented in
Section~\ref{sec:formal:Promela Parable: dynticks and Preemptible RCU}
turned out to
have a much simpler alternative implementation, as discussed in
Section~\ref{sec:formal:Simplicity Avoids Formal Verification}.
All else being equal, a simpler implementation is much better than
a proof of correctness for a complex implementation!

And the open challenge to those working on formal verification techniques
and systems is to prove this summary wrong!
To assist in this task, Verification Challenge~6 is now
available~\cite{PaulEMcKenney2017VerificationChallenge6}.
Have at it!!!

\section{Choosing a Validation Plan}
\label{sec:formal:Choosing a Validation Plan}

What sort of validation should you use for your project?

As is often the case in software in particular and in engineering
in general, the answer is ``it depends''.

Note that neither running a test nor undertaking formal verification
will change your project.
At best, such effort have an indirect effect by locating a bug that
is later fixed.
Nevertheless, fixing a bug might prevent inconvenience, monetary loss,
property damage, or even loss of life.
Clearly, this sort of indirect effect can be extremely valuable.

Unfortunately, as we have seen, it is difficult to predict whether or
not a given validation effort will find important bugs.
It is therefore all too easy to invest too little---or even to fail
to invest at all, especially if development estimates proved overly
optimistic or budgets unexpectedly tight, conditions which almost
always come into play in real-world software projects.

The decision to nevertheless invest in validation is often forced by
experienced people with forceful personalities.
But this is no guarantee, given that other stakeholders might also
have forceful personalities.
Worse yet, these other stakeholders might bring stories of expensive
validation efforts that nevertheless allowed embarrassing bugs to
escape to the end users.
So although a scarred, grey-haired, and grouchy veteran might carry
the day, a more organized approach would perhaps be more useful.

Fortunately, there is a strictly financial analog to investments in
validation, and that is the insurance policy.

Both insurance policies and validation efforts require consistent
up-front investments, and both defend against disasters that might
or might not ever happen.
Furthermore, both have exclusions of various types.
For example, insurance policies for coastal areas might exclude
damages due to tidal waves, while on the other hand we have seen
that there is not yet any validation methodology that can find
each and every bug.

In addition, it is possible to over-invest in both insurance and
in validation.
For but one example, a validation plan that consumed the entire
development budget would be just as pointless as would an insurance
policy that covered the Sun going nova.

\begin{table}[tbhp]
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.2}\centering\small
\begin{tabular}{lrrrr}\toprule
	& LoC & Test LoC & Total LoC & \% Test \\
v2.6.12 & 1151 & 0 & 1151 & 0.0 \\
v2.6.13 & 1151 & 0 & 1151 & 0.0 \\
v2.6.14 & 1397 & 0 & 1397 & 0.0 \\
v2.6.15 & 1459 & 513 & 1972 & 26.0 \\
v2.6.16 & 1270 & 598 & 1868 & 32.0 \\
v2.6.17 & 1287 & 605 & 1892 & 32.0 \\
v2.6.18 & 1308 & 718 & 2026 & 35.4 \\
v2.6.19 & 1613 & 1001 & 2614 & 38.3 \\
v2.6.20 & 1615 & 1004 & 2619 & 38.3 \\
v2.6.21 & 1615 & 1004 & 2619 & 38.3 \\
v2.6.22 & 1617 & 1001 & 2618 & 38.2 \\
v2.6.23 & 1616 & 999 & 2615 & 38.2 \\
v2.6.24 & 1649 & 995 & 2644 & 37.6 \\
v2.6.25 & 3532 & 995 & 4527 & 22.0 \\
v2.6.26 & 3523 & 999 & 4522 & 22.1 \\
v2.6.27 & 4361 & 1157 & 5518 & 21.0 \\
v2.6.28 & 4552 & 1157 & 5709 & 20.3 \\
v2.6.29 & 6863 & 1227 & 8090 & 15.2 \\
v2.6.30 & 6911 & 1236 & 8147 & 15.2 \\
v2.6.31 & 7012 & 1236 & 8248 & 15.0 \\
v2.6.32 & 4876 & 1257 & 6133 & 20.5 \\
v2.6.33 & 5641 & 1296 & 6937 & 18.7 \\
v2.6.34 & 6235 & 1380 & 7615 & 18.1 \\
v2.6.35 & 6470 & 1382 & 7852 & 17.6 \\
v2.6.36 & 6716 & 1381 & 8097 & 17.1 \\
v2.6.37 & 7513 & 1390 & 8903 & 15.6 \\
v2.6.38 & 8217 & 1638 & 9855 & 16.6 \\
v2.6.39 & 8237 & 1637 & 9874 & 16.6 \\
\bottomrule
\end{tabular}
\caption{Linux-Kernel RCU Test Code (v2.6)}
\label{tab:formal:Linux-Kernel RCU Test Code (v2.6)}
\end{table}

\begin{table}[tbhp]
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.2}\centering\small
\begin{tabular}{lrrrr}\toprule
     & LoC & Test LoC & Total LoC & \% Test \\
v3.0 & 9422 & 1633 & 11055 & 14.8 \\
v3.1 & 9422 & 1631 & 11053 & 14.8 \\
v3.2 & 9594 & 1622 & 11216 & 14.5 \\
v3.3 & 10126 & 1833 & 11959 & 15.3 \\
v3.4 & 10668 & 1914 & 12582 & 15.2 \\
v3.5 & 11303 & 2141 & 13444 & 15.9 \\
v3.6 & 11313 & 2157 & 13470 & 16.0 \\
v3.7 & 11349 & 2192 & 13541 & 16.2 \\
v3.8 & 11965 & 2174 & 14139 & 15.4 \\
v3.9 & 12165 & 2216 & 14381 & 15.4 \\
v3.10 & 12211 & 2216 & 14427 & 15.4 \\
v3.11 & 11120 & 2177 & 13297 & 16.4 \\
v3.12 & 11527 & 2139 & 13666 & 15.7 \\
v3.13 & 11739 & 2145 & 13884 & 15.4 \\
v3.14 & 11942 & 3428 & 15370 & 22.3 \\
v3.15 & 12007 & 3882 & 15889 & 24.4 \\
v3.16 & 12222 & 4074 & 16296 & 25.0 \\
v3.17 & 12465 & 4088 & 16553 & 24.7 \\
v3.18 & 13065 & 4238 & 17303 & 24.5 \\
v3.19 & 13190 & 4230 & 17420 & 24.3 \\
v4.0 & 13148 & 4265 & 17413 & 24.5 \\
v4.1 & 13396 & 4290 & 17686 & 24.3 \\
v4.2 & 13408 & 4348 & 17756 & 24.5 \\
v4.3 & 13557 & 4364 & 17921 & 24.4 \\
v4.4 & 14192 & 4363 & 18555 & 23.5 \\
v4.5 & 14359 & 4421 & 18780 & 23.5 \\
v4.6 & 14461 & 4423 & 18884 & 23.4 \\
v4.7 & 14612 & 5480 & 20092 & 27.3 \\
v4.8 & 14688 & 5549 & 20237 & 27.4 \\
v4.9 & 14762 & 5527 & 20289 & 27.2 \\
v4.10 & 14831 & 5541 & 20372 & 27.2 \\
v4.11 & 15030 & 5559 & 20589 & 27.0 \\
v4.12 & 17508 & 5588 & 23096 & 24.2 \\
v4.13 & 15884 & 5663 & 21547 & 26.3 \\
v4.14 & 15781 & 5712 & 21493 & 26.6 \\
v4.15 & 15857 & 5726 & 21583 & 26.5 \\
v4.16 & 15777 & 5670 & 21447 & 26.4 \\
v4.17 & 15774 & 5703 & 21477 & 26.6 \\
v4.18 & 15751 & 5756 & 21507 & 26.8 \\
v4.19 & 15974 & 5967 & 21941 & 27.2 \\
v4.20 & 15519 & 6054 & 21573 & 28.1 \\
\bottomrule
\end{tabular}
\caption{Linux-Kernel RCU Test Code (v3-4)}
\label{tab:formal:Linux-Kernel RCU Test Code (v3-4)}
\end{table}

One approach is to devote a given fraction of the software budget to
validation, with that fraction depending on the criticality of the
software, so that safety-critical avionics software might grant a
larger fraction of its budget to validation than would a homework
assignment.
Where available, experience from prior similar projects should be
brought to bear.
However, it is necessary to structure the project so that the validation
investment starts when the project does, otherwise the inevitable overruns
in spending on coding will crowd out the validation effort.

Staffing start-up projects with experienced people can result in
overinvestment in validation efforts.
Just as it is possible to go broke buying too much insurance, it is
possible to kill a project by investing too much in testing.
This is especially the case for first-of-a-kind projects where it is
not yet clear which use cases will be important, in which case testing
for all possible use cases will be a possibly fatal waste of time,
energy, and funding.

However, as such a start-up project grows, so grows the need for
validation.
Users who were once willing to forgive the occasional failure for a
task that could not be done any other way may be less forgiving
as that task becomes more routine.
Managing this shift in investment can be extremely challenging,
especially in the all-too-common case where the users are unwilling
or unable to disclose the exact nature of their use case.
In these cases, it is critically important to reverse-engineer the
use cases from bug reports and from whatever might be gleaned from
discussions with the users.
As these use cases become understood, use of continuous integration
can help reduce the cost of finding and fixing any bugs located.

One example evolution of a software project's use of validation is
shown in
\crefthro{tab:formal:Linux-Kernel RCU Test Code (v2.6)}
{tab:formal:Linux-Kernel RCU Test Code (v3-4)}.
As can be seen in \cref{tab:formal:Linux-Kernel RCU Test Code (v2.6)},
Linux-kernel RCU didn't have any validation code whatsoever until Linux
kernel v2.6.16, which was released more than two years after RCU was
accepted into the kernel.
The test suite achieved its peak fraction of the total lines of code
in Linux kernel v2.6.19-v2.6.21.
This fraction decreased sharply with the acceptance of preemptible RCU
for real-time applications in v2.6.25.
This decrease was due to the fact that the RCU API was identical
in the preemptible and non-preemptible variants of RCU.
This in turn meant that the existing test suite applied to both variants,
so that there was no need to expand the tests.

Moving on to
\cref{tab:formal:Linux-Kernel RCU Test Code (v3-4)},
we can see that the RCU code base expanded significantly, but that the
corresponding validation code expanded even more dramatically.
Linux kernel v3.5 added tests for the \co{rcu_barrier()} API, closing
a long-standing hole in test coverage.
Linux kernel v3.14 added scripting that automated testing and also
analysis of test results, moving RCU towards continuous integration.
Linux kernel v4.7 added a performance validation suite for RCU's update-side
primitives.
Numerous other changes may be found in the Linux kernel's \co{git} archives.
% rcutorture
% v2.6.15: First torture test
% v2.6.19: SRCU: Plugin architecture avoids test-code explosion.
% v2.6.19-21: Peak test fraction.
% v2.6.25: preemptible RCU, consistent API avoids added test code.
% v3.4: Add tests for RCU CPU stall warnings.
% v3.5: Add tests for rcu_barrier(). *
% v3.14: Add rcutorture scripting automating tests and results analysis. *
% v3.15: Add support for multiple torture-tests suites for locktorture.
% v3.16: Add support for conditional grace-period primitives.
% v4.7: Add update-side performance validation suite. *
% v5.0: nolibc was briefly in the rcutorture scripting directory.

We have established that the validation budget varies from one project
to the next, and also over the lifetime of any given project.
But how should the validation investment be split between testing and
formal verification?

This question is being answered naturally as compilers adopt increasingly
aggressive formal-verification techniques into their diagnostics and
as formal-verification tools continue to mature.
In the meantime, the best practice is to focus first on testing and to
reserve explicit work on formal verification for those portions of the
project that are not well-served by testing, and that have exceptional
needs for robustness.
For example, Linux-kernel RCU relies primarily on testing, but has
made occasional use of formal verification as discussed in this chapter.

In short, choosing a validation plan for concurrent software remains
more an art than a science, let alone a field of engineering.
However, there is every reason to expect that ordered approaches
will continue to become more prevalent.
